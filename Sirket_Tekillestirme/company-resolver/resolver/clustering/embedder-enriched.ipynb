{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yagizerdem/Documents/vs-code/company-resolver/resolver/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/shuffled_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    parent_data = json.load(f)\n",
    "\n",
    "parent_data = pd.json_normalize(parent_data)\n",
    "\n",
    "print(len(parent_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2581\n",
      "2581\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/subsidiary_companies_with_prod_descs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sub_data = json.load(f)\n",
    "\n",
    "sub_data = pd.json_normalize(sub_data)\n",
    "\n",
    "print(len(sub_data))\n",
    "\n",
    "sub_data = sub_data[~sub_data[\"info.product_description\"].isna()]\n",
    "\n",
    "print(len(sub_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 355, 45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singular_data = parent_data[~parent_data[\"company_name\"].str.contains(\"holding\", case=False) &\n",
    "                            ~parent_data[\"company_name\"].str.contains(\"grup\", case=False) &\n",
    "                            ~parent_data[\"company_name\"].str.contains(\"grubu\", case=False)]\n",
    "\n",
    "only_parent_data = parent_data[parent_data[\"company_name\"].str.contains(\"holding\", case=False) |\n",
    "                               parent_data[\"company_name\"].str.contains(\"grup\", case=False) |\n",
    "                               parent_data[\"company_name\"].str.contains(\"grubu\", case=False)]\n",
    "\n",
    "\n",
    "len(parent_data), len(singular_data), len(only_parent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_out_subsidiries_in_df = singular_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in only_parent_data.iterrows():\n",
    "    for inner_idx, inner_row in sub_data.iterrows():\n",
    "        if row[\"info.company_name\"] == inner_row[\"parent_name\"]:\n",
    "            inner_df = pd.DataFrame({\"info.company_name\": [inner_row[\"info.company_name\"]],\n",
    "                                     \"info.affiliations.parents\": [[inner_row[\"parent_name\"]]],\n",
    "                                     \"info.product_description\": [inner_row[\"info.product_description\"]]\n",
    "                                     })\n",
    "            \n",
    "            holdings_out_subsidiries_in_df = pd.concat([holdings_out_subsidiries_in_df, inner_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_out_subsidiries_in_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to cache the embedded data\n",
    "embedding_cache = {}\n",
    "\n",
    "# Check for existing embedding files and load them into the cache\n",
    "if os.path.isfile(\"gte-multilingual-base.parquet\"):\n",
    "    embedding_cache['gte-multilingual-base'] = pd.read_parquet('gte-multilingual-base.parquet', engine='pyarrow')\n",
    "\n",
    "if os.path.isfile(\"paraphrase-multilingual-MiniLM-L12-v2.parquet\"):\n",
    "    embedding_cache[\"paraphrase-multilingual-MiniLM-L12-v2\"] = pd.read_parquet('paraphrase-multilingual-MiniLM-L12-v2.parquet', engine='pyarrow')\n",
    "\n",
    "if os.path.isfile(\"LaBSE.parquet\"):\n",
    "    embedding_cache[\"LaBSE\"] = pd.read_parquet('LaBSE.parquet', engine='pyarrow')\n",
    "\n",
    "if os.path.isfile(\"bge-m3.parquet\"):\n",
    "    embedding_cache[\"bge-m3\"] = pd.read_parquet('bge-m3.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(model_name, dataframe, columns_to_embed):\n",
    "    # Check if we already have embeddings for this model cached\n",
    "    if model_name.split(\"/\")[1] in embedding_cache:\n",
    "        print(f\"Using cached embeddings for model: {model_name.split('/')[1]}\")\n",
    "        return embedding_cache[model_name.split(\"/\")[1]]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load the sentence transformer model and move it to the GPU if available\n",
    "    model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "    \n",
    "    # Combine the text from specified columns into a single string per row\n",
    "    combined_texts = dataframe[columns_to_embed].astype(str).agg(' '.join, axis=1).tolist()\n",
    "    \n",
    "    # Generate the embeddings for the combined texts, and run them on the GPU if available\n",
    "    embeddings = model.encode(combined_texts, show_progress_bar=True, normalize_embeddings=True, device=device)\n",
    "\n",
    "    # Create a DataFrame with companies and embeddings\n",
    "    embedded_df = dataframe.copy()\n",
    "    embedded_df[\"metadata.embedding\"] = list(embeddings)\n",
    "\n",
    "    # Cache the embeddings for future use\n",
    "    embedding_cache[model_name.split(\"/\")[1]] = embedded_df\n",
    "\n",
    "    # Release the GPU memory by deleting the model and clearing the cache\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return embedded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"sentence-transformers/LaBSE\", \"Alibaba-NLP/gte-multilingual-base\", \n",
    "               \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"BAAI/bge-m3\"]\n",
    "\n",
    "columns_to_embed = [\"info.product_description\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    embedded_df = embed_data(model_name, holdings_out_subsidiries_in_df, columns_to_embed)\n",
    "    short_name = model_name.split(\"/\")[1]\n",
    "    embedded_df.to_parquet(f'{short_name}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_DBSCAN_PCA(dataframe, eps, metric, reduced_dims=None):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on embeddings with optional PCA dimensionality reduction.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas DataFrame\n",
    "        DataFrame containing embeddings in 'metadata.embedding' column\n",
    "    eps : float\n",
    "        The maximum distance between two samples for one to be considered in the neighborhood of the other\n",
    "    metric : str\n",
    "        Distance metric to use ('cosine', 'euclidean', etc.)\n",
    "    reduced_dims : int, optional\n",
    "        Number of dimensions to reduce to using PCA before clustering\n",
    "        If None, clustering is performed on the original embeddings\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with cluster assignments\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe to avoid modifying it\n",
    "    df_copy = dataframe.copy()\n",
    "    \n",
    "    # Extract the embeddings from the dataframe\n",
    "    embeddings = np.vstack(df_copy['metadata.embedding'].values) # Stack the embeddings into a 2D array\n",
    "    \n",
    "    # Apply PCA dimensionality reduction if requested\n",
    "    if reduced_dims is not None:\n",
    "        print(f\"Applying PCA dimensionality reduction to {reduced_dims} dimensions...\")\n",
    "        pca = PCA(n_components=reduced_dims, random_state=42)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Print the explained variance ratio\n",
    "        explained_variance = sum(pca.explained_variance_ratio_) * 100\n",
    "        print(f\"PCA explained variance: {explained_variance:.2f}%\")\n",
    "        \n",
    "        # Perform DBSCAN clustering on the reduced embeddings\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=1, metric=metric)\n",
    "        df_copy['metadata.cluster'] = dbscan.fit_predict(reduced_embeddings)\n",
    "    else:\n",
    "        # Perform DBSCAN clustering on the original embeddings\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=1, metric=metric)\n",
    "        df_copy['metadata.cluster'] = dbscan.fit_predict(embeddings)\n",
    "    \n",
    "    # For each cluster, rename the cluster name to the first company in that cluster\n",
    "    cluster_names = {}\n",
    "    for cluster in df_copy['metadata.cluster'].unique():\n",
    "        # Find the first company name for the cluster\n",
    "        first_company = df_copy[df_copy['metadata.cluster'] == cluster].iloc[0]['info.company_name']\n",
    "        cluster_names[cluster] = first_company\n",
    "    \n",
    "    # Replace cluster IDs with the first company name\n",
    "    df_copy['metadata.cluster'] = df_copy['metadata.cluster'].map(cluster_names)\n",
    "    \n",
    "    # Return the dataframe with the relevant columns\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_DBSCAN(dataframe, eps, metric, reduced_dims=None):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on embeddings with optional t-SNE dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas DataFrame\n",
    "        DataFrame containing embeddings in 'metadata.embedding' column\n",
    "    eps : float\n",
    "        The maximum distance between two samples for one to be considered in the neighborhood of the other\n",
    "    metric : str\n",
    "        Distance metric to use ('cosine', 'euclidean', etc.)\n",
    "    reduced_dims : int, optional\n",
    "        Number of dimensions to reduce to using t-SNE before clustering\n",
    "        If None, clustering is performed on the original embeddings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with cluster assignments\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe to avoid modifying it\n",
    "    df_copy = dataframe.copy()\n",
    "    \n",
    "    # Extract the embeddings from the dataframe\n",
    "    embeddings = np.vstack(df_copy['metadata.embedding'].values)  # Stack the embeddings into a 2D array\n",
    "    \n",
    "    # Apply t-SNE dimensionality reduction if requested\n",
    "    if reduced_dims is not None:\n",
    "        print(f\"Applying t-SNE dimensionality reduction to {reduced_dims} dimensions...\")\n",
    "\n",
    "        if reduced_dims <= 3:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='barnes_hut')\n",
    "        else:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='exact')\n",
    "\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        # Print the KL divergence (information loss)\n",
    "        print(f\"t-SNE KL divergence (information loss): {tsne.kl_divergence_:.4f}\")\n",
    "        \n",
    "        # Perform DBSCAN clustering on the reduced embeddings\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=1, metric=metric)\n",
    "        df_copy['metadata.cluster'] = dbscan.fit_predict(reduced_embeddings)\n",
    "    else:\n",
    "        # Perform DBSCAN clustering on the original embeddings\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=1, metric=metric)\n",
    "        df_copy['metadata.cluster'] = dbscan.fit_predict(embeddings)\n",
    "    \n",
    "    # For each cluster, rename the cluster name to the first company in that cluster\n",
    "    cluster_names = {}\n",
    "    for cluster in df_copy['metadata.cluster'].unique():\n",
    "        # Find the first company name for the cluster\n",
    "        first_company = df_copy[df_copy['metadata.cluster'] == cluster].iloc[0]['info.company_name']\n",
    "        cluster_names[cluster] = first_company\n",
    "    \n",
    "    # Replace cluster IDs with the first company name\n",
    "    df_copy['metadata.cluster'] = df_copy['metadata.cluster'].map(cluster_names)\n",
    "    \n",
    "    # Return the dataframe with the relevant columns\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_HDBSCAN(dataframe, min_cluster_size, min_samples, metric, eps, reduced_dims=None):\n",
    "    \"\"\"\n",
    "    Perform HDBSCAN clustering on embeddings with optional t-SNE dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas DataFrame\n",
    "        DataFrame containing embeddings in 'metadata.embedding' column\n",
    "    min_cluster_size : int\n",
    "        Minimum size of clusters\n",
    "    min_samples : int\n",
    "        Minimum number of samples in the neighborhood to be considered a core point\n",
    "    metric : str\n",
    "        Distance metric to use ('cosine', 'euclidean', etc.)\n",
    "    eps : float\n",
    "        Cluster selection epsilon\n",
    "    reduced_dims : int, optional\n",
    "        Number of dimensions to reduce to using t-SNE before clustering\n",
    "        If None, clustering is performed on the original embeddings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with cluster assignments\n",
    "    \"\"\"\n",
    "    df_copy = dataframe.copy()\n",
    "    embeddings = np.vstack(df_copy['metadata.embedding'].values).astype(np.float64)\n",
    "    \n",
    "    # Apply t-SNE dimensionality reduction if requested\n",
    "    if reduced_dims is not None:\n",
    "        print(f\"Applying t-SNE dimensionality reduction to {reduced_dims} dimensions...\")\n",
    "\n",
    "        if reduced_dims <= 3:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='barnes_hut')\n",
    "        else:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='exact')\n",
    "            \n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        # Use the reduced embeddings for clustering\n",
    "        if metric == 'cosine':\n",
    "            distance_matrix = cosine_distances(reduced_embeddings).astype(np.float64)\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                metric='precomputed',\n",
    "                gen_min_span_tree=True,\n",
    "                cluster_selection_method='eom',\n",
    "                cluster_selection_epsilon=eps\n",
    "            )\n",
    "            cluster_labels = clusterer.fit_predict(distance_matrix)\n",
    "        else:\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                metric=metric,\n",
    "                gen_min_span_tree=True,\n",
    "                cluster_selection_method='eom',\n",
    "                cluster_selection_epsilon=eps\n",
    "            )\n",
    "            cluster_labels = clusterer.fit_predict(reduced_embeddings)\n",
    "    else:\n",
    "        # Use original embeddings for clustering (original behavior)\n",
    "        if metric == 'cosine':\n",
    "            distance_matrix = cosine_distances(embeddings).astype(np.float64)\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                metric='precomputed',\n",
    "                gen_min_span_tree=True,\n",
    "                cluster_selection_method='eom',\n",
    "                cluster_selection_epsilon=eps\n",
    "            )\n",
    "            cluster_labels = clusterer.fit_predict(distance_matrix)\n",
    "        else:\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                metric=metric,\n",
    "                gen_min_span_tree=True,\n",
    "                cluster_selection_method='eom',\n",
    "                cluster_selection_epsilon=eps\n",
    "            )\n",
    "            cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    \n",
    "    df_copy['cluster_id'] = cluster_labels\n",
    "    \n",
    "    # Step 1: Separate clusters and outliers\n",
    "    clustered = df_copy[df_copy['cluster_id'] != -1].copy()\n",
    "    outliers = df_copy[df_copy['cluster_id'] == -1].copy()\n",
    "    \n",
    "    # Step 2: Calculate cluster centroids (using original embeddings for consistent distance calculation)\n",
    "    centroids = clustered.groupby('cluster_id')['metadata.embedding'].apply(\n",
    "        lambda x: np.mean(np.vstack(x.values), axis=0)\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Step 3: Assign each outlier to the closest centroid\n",
    "    for idx, row in outliers.iterrows():\n",
    "        embedding = np.array(row['metadata.embedding']).reshape(1, -1)\n",
    "        closest_cluster = min(\n",
    "            centroids.items(),\n",
    "            key=lambda item: cosine_distances(embedding, item[1].reshape(1, -1))[0][0]\n",
    "        )[0]\n",
    "        \n",
    "        # Assign this outlier to the closest cluster\n",
    "        df_copy.at[idx, 'cluster_id'] = closest_cluster\n",
    "        \n",
    "        # Update centroid with new point (running mean)\n",
    "        cluster_points = df_copy[df_copy['cluster_id'] == closest_cluster]['metadata.embedding'].values\n",
    "        updated_centroid = np.mean(np.vstack(cluster_points), axis=0)\n",
    "        centroids[closest_cluster] = updated_centroid\n",
    "    \n",
    "    # Step 4: Rename clusters by first company name\n",
    "    cluster_names = {}\n",
    "    for cluster in df_copy['cluster_id'].unique():\n",
    "        first_company = df_copy[df_copy['cluster_id'] == cluster].iloc[0]['info.company_name']\n",
    "        cluster_names[cluster] = first_company\n",
    "    \n",
    "    df_copy['metadata.cluster'] = df_copy['cluster_id'].map(cluster_names)\n",
    "    df_copy.drop(columns=['cluster_id'], inplace=True)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cluster_members(df):\n",
    "  # Group the dataframe by 'cluster' and aggregate the company names into lists\n",
    "  cluster_members = df.groupby('metadata.cluster')['info.company_name'].apply(list).reset_index()\n",
    "    \n",
    "  # Rename the columns to 'cluster_name' and 'members'\n",
    "  cluster_members.columns = ['metadata.cluster', 'members']\n",
    "    \n",
    "  return cluster_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette_score(df, metric, reduced_dims=None):\n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "    \n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return -1  # Cannot compute with only one cluster\n",
    "    \n",
    "    if reduced_dims is not None:\n",
    "        print(f\"Applying t-SNE dimensionality reduction to {reduced_dims} dimensions for silhouette score...\")\n",
    "        if reduced_dims <= 3:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='barnes_hut')\n",
    "        else:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='exact')\n",
    "        embeddings = tsne.fit_transform(embeddings)\n",
    "        print(f\"t-SNE KL divergence (information loss): {tsne.kl_divergence_:.4f}\")\n",
    "\n",
    "    score = silhouette_score(embeddings, labels, metric=metric if reduced_dims is None else 'euclidean')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gap_statistic(df, n_refs=5):\n",
    "    random_state = 42\n",
    "    \n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "\n",
    "    def calculate_dispersion_cosine(embeddings, labels):\n",
    "        \"\"\"Calculate the sum of squared cosine distances from points to their cluster centroids.\"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        dispersion = 0\n",
    "        for label in unique_labels:\n",
    "            cluster_points = embeddings[labels == label]\n",
    "            if len(cluster_points) > 1:\n",
    "                centroid = np.mean(cluster_points, axis=0)\n",
    "                # Compute cosine distances between points and their cluster centroids\n",
    "                distances = cosine_distances(cluster_points, centroid.reshape(1, -1))\n",
    "                dispersion += np.sum(distances ** 2)\n",
    "        return dispersion\n",
    "\n",
    "    # Calculate dispersion for actual data using cosine distances\n",
    "    actual_dispersion = calculate_dispersion_cosine(embeddings, labels)\n",
    "    \n",
    "    # Generate reference datasets and calculate their dispersion\n",
    "    ref_disps = np.zeros(n_refs)\n",
    "    for i in range(n_refs):\n",
    "        # Create a random reference dataset with a different seed for each run\n",
    "        np.random.seed(random_state + i)  # Different seed for each reference dataset\n",
    "        random_ref = np.random.uniform(low=np.min(embeddings, axis=0), high=np.max(embeddings, axis=0), size=embeddings.shape)\n",
    "        ref_kmeans = KMeans(n_clusters=len(np.unique(labels)), random_state=random_state).fit(random_ref)\n",
    "        ref_disps[i] = calculate_dispersion_cosine(random_ref, ref_kmeans.labels_)\n",
    "\n",
    "    # Calculate the gap statistic\n",
    "    gap_stat = np.log(np.mean(ref_disps)) - np.log(actual_dispersion)\n",
    "    return gap_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_sum_of_squares_cosine(df):\n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    total_ess = 0\n",
    "    for label in unique_labels:\n",
    "        cluster_points = embeddings[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            # Compute cosine distances from points to centroid\n",
    "            distances = cosine_distances(cluster_points, centroid.reshape(1, -1))\n",
    "            total_ess += np.sum(distances ** 2)\n",
    "    \n",
    "    return total_ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_davies_bouldin_index(df, reduced_dims=None):\n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "    \n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return -1  # Undefined for one cluster\n",
    "\n",
    "    if reduced_dims is not None:\n",
    "        print(f\"Applying t-SNE dimensionality reduction to {reduced_dims} dimensions for Davies-Bouldin index...\")\n",
    "        if reduced_dims <= 3:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='barnes_hut')\n",
    "        else:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='exact')\n",
    "        embeddings = tsne.fit_transform(embeddings)\n",
    "        print(f\"t-SNE KL divergence (information loss): {tsne.kl_divergence_:.4f}\")\n",
    "    \n",
    "    score = davies_bouldin_score(embeddings, labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_calinski_harabasz_index(df, reduced_dims=None):\n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "    \n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return -1  # Undefined for one cluster\n",
    "\n",
    "    if reduced_dims is not None:\n",
    "        print(f\"Applying t-SNE dimensionality reduction to {reduced_dims} dimensions for Calinski-Harabasz index...\")\n",
    "        if reduced_dims <= 3:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='barnes_hut')\n",
    "        else:\n",
    "            tsne = TSNE(n_components=reduced_dims, random_state=42, n_jobs=-1, method='exact')\n",
    "        embeddings = tsne.fit_transform(embeddings)\n",
    "        print(f\"t-SNE KL divergence (information loss): {tsne.kl_divergence_:.4f}\")\n",
    "    \n",
    "    score = calinski_harabasz_score(embeddings, labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to return all three scores\n",
    "def run_tests(df, metric, reduced_dims=None):\n",
    "    silhouette = calculate_silhouette_score(df, metric, reduced_dims=reduced_dims)\n",
    "    gap_stat = calculate_gap_statistic(df)\n",
    "    ess = calculate_error_sum_of_squares_cosine(df)\n",
    "    dbi = calculate_davies_bouldin_index(df, reduced_dims=reduced_dims)\n",
    "    chi = calculate_calinski_harabasz_index(df, reduced_dims=reduced_dims)\n",
    "    \n",
    "    return silhouette, gap_stat, ess, dbi, chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_evaluate(model_name, dataframe, columns_to_embed, eps=0.1, metric='cosine', reduced_dims=None):\n",
    "    # Step 1: Embed the data\n",
    "    embedded_df = embed_data(model_name, dataframe, columns_to_embed)\n",
    "    \n",
    "    # Step 2: Cluster the embedded data using DBSCAN\n",
    "    clustered_df = cluster_DBSCAN(embedded_df, eps=eps, metric=metric, reduced_dims=reduced_dims)\n",
    "    # clustered_df = cluster_HDBSCAN(embedded_df, min_cluster_size=2, min_samples=1, metric=metric, eps=eps, reduced_dims=reduced_dims)\n",
    "    \n",
    "    # Step 3: Evaluate clustering with silhouette score, gap statistic, and error sum of squares\n",
    "    silhouette, gap_stat, ess, dbi, chi = run_tests(clustered_df, metric, reduced_dims=reduced_dims)\n",
    "    \n",
    "    # Return the clustered dataframe along with the evaluation metrics\n",
    "    return clustered_df, silhouette, gap_stat, ess, dbi, chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_embed = [\"info.product_description\"]\n",
    "\n",
    "model_names = [\"BAAI/bge-m3\", \"Alibaba-NLP/gte-multilingual-base\", \n",
    "               \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"sentence-transformers/LaBSE\"]\n",
    "\n",
    "eps_values = [0] # 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_and_eps(dataframe, columns_to_embed, model_names, eps_values, reduced_dims):\n",
    "    results = []\n",
    "\n",
    "    # Outer loop for model names\n",
    "    for model_name in model_names:\n",
    "        # Inner loop for eps values\n",
    "        for eps in eps_values:\n",
    "            # Run the cluster_and_evaluate function with the current model and eps\n",
    "            clustered_df, silhouette, gap_stat, ess, dbi, chi = cluster_and_evaluate(\n",
    "                model_name, dataframe, columns_to_embed, eps=eps, metric=\"euclidean\", reduced_dims=reduced_dims\n",
    "            )\n",
    "            \n",
    "            # Append the results as a row in the list\n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'eps': eps,\n",
    "                'silhouette_score': silhouette,\n",
    "                'gap_statistic': gap_stat,\n",
    "                'error_sum_of_squares': ess,\n",
    "                'dbi': dbi,\n",
    "                'chi': chi\n",
    "            })\n",
    "    \n",
    "    # Convert the results list to a DataFrame and return it\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_models_and_eps(holdings_out_subsidiries_in_df, columns_to_embed, model_names, eps_values, reduced_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df, silhouette, gap_stat, ess, dbi, chi = cluster_and_evaluate(\"Alibaba-NLP/gte-multilingual-base\", holdings_out_subsidiries_in_df, columns_to_embed, eps=2.0, metric='euclidean', reduced_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{silhouette:.4f} {gap_stat:.4f} {ess:.4f} {dbi:.4f} {chi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings and clusters\n",
    "embeddings = clustered_df['metadata.embedding'].tolist()\n",
    "clusters = clustered_df['metadata.cluster'].tolist()\n",
    "company_names = clustered_df['info.company_name'].tolist()  # Extract company names\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, max_iter=1000)\n",
    "\n",
    "# Fit and transform the embeddings using t-SNE\n",
    "tsne_embeddings = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a new dataframe with the t-SNE components\n",
    "df_tsne_embeddings = pd.DataFrame(tsne_embeddings, columns=['TSNE1', 'TSNE2'])\n",
    "df_tsne_embeddings['cluster'] = clusters  # Add the cluster information for coloring\n",
    "df_tsne_embeddings['company_name'] = company_names  # Add company names for hover data\n",
    "\n",
    "# Get unique clusters\n",
    "unique_clusters = sorted(df_tsne_embeddings['cluster'].unique())\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "print(f\"Total number of clusters: {num_clusters}\")\n",
    "\n",
    "# Function to generate distinct colors\n",
    "def generate_distinct_colors(n):\n",
    "    \"\"\"Generate visually distinct colors by cycling through hues and varying saturation/value\"\"\"\n",
    "    colors = []\n",
    "    \n",
    "    # Use HSV color space for better control\n",
    "    for i in range(n):\n",
    "        # Cycle through the hue spectrum\n",
    "        h = i / n\n",
    "        # Alternate between high and medium saturation\n",
    "        s = 0.8 if i % 2 == 0 else 0.6\n",
    "        # Alternate between high and medium value (brightness)\n",
    "        v = 0.9 if i % 3 == 0 else 0.7\n",
    "        \n",
    "        # Convert HSV to RGB\n",
    "        rgb = plt.cm.hsv(h)\n",
    "        colors.append(rgb)\n",
    "    \n",
    "    # Shuffle colors to avoid having similar colors next to each other\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    return colors\n",
    "\n",
    "# Generate distinct colors for all clusters\n",
    "distinct_colors = generate_distinct_colors(num_clusters)\n",
    "\n",
    "# Create a custom color map\n",
    "cluster_color_map = {cluster: distinct_colors[i] for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# Create the interactive plotly visualization with optimizations\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.io as pio\n",
    "    \n",
    "    # Set renderer to 'browser' to avoid notebook rendering issues\n",
    "    pio.renderers.default = 'browser'\n",
    "    \n",
    "    # Create a color column for plotly\n",
    "    df_tsne_embeddings['color'] = df_tsne_embeddings['cluster'].map(\n",
    "        {cluster: mcolors.to_hex(cluster_color_map[cluster]) for cluster in unique_clusters}\n",
    "    )\n",
    "    \n",
    "    # OPTIMIZATION: Create one trace per cluster instead of one trace per point\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for cluster in sorted(unique_clusters):\n",
    "        cluster_data = df_tsne_embeddings[df_tsne_embeddings['cluster'] == cluster]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cluster_data['TSNE1'],\n",
    "            y=cluster_data['TSNE2'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=mcolors.to_hex(cluster_color_map[cluster]),\n",
    "                size=8,\n",
    "                line=dict(width=1, color='DarkSlateGrey')\n",
    "            ),\n",
    "            name=f'Cluster {cluster}',\n",
    "            text=cluster_data['company_name'],  # Use text for basic hover\n",
    "            hoverinfo='text',\n",
    "            hovertemplate='<b>%{text}</b><br>Cluster: ' + str(cluster) + '<extra></extra>',\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='t-SNE Visualization of Embeddings',\n",
    "        legend_title_text='Cluster',\n",
    "        width=1000,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # If there are too many clusters, hide the legend\n",
    "    if num_clusters > 30:\n",
    "        fig.update_layout(showlegend=False)\n",
    "    \n",
    "    # OPTIMIZATION: Save with lower precision and without full_html option\n",
    "    try:\n",
    "        fig.write_html(\n",
    "            \"tsne_clusters_visualization_optimized.html\",\n",
    "            include_plotlyjs='cdn',  # Use CDN for plotly.js\n",
    "            full_html=False,  # Don't include the full HTML wrapper\n",
    "            include_mathjax=False,  # No MathJax\n",
    "            post_script=None,  # No additional scripts\n",
    "            validate=False  # Skip validation for speed\n",
    "        )\n",
    "        print(\"Optimized interactive visualization saved to 'tsne_clusters_visualization_optimized.html'\")\n",
    "        print(\"Open this file in your browser for the interactive plot\")\n",
    "\n",
    "        fig.update_layout(title=None)\n",
    "        fig.update_layout(\n",
    "            margin=dict(l=0, r=0, t=0, b=0),  # Remove all margins\n",
    "            paper_bgcolor='rgba(0,0,0,0)',   # Transparent background\n",
    "        )\n",
    "\n",
    "        pio.write_image(fig, \"tsne_clusters_visualization_optimized.svg\")\n",
    "        print(\"Visualization saved as 'tsne_clusters_visualizatsne_clusters_visualization_optimizedtion.svg'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save HTML or SVG file: {e}\")\n",
    "        # Fallback to showing the plot if available\n",
    "        try:\n",
    "            fig.show()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Plotly not available. Install with 'pip install plotly' for interactive visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plotly visualization with annotations for selected companies\n",
    "try:\n",
    "    # Set renderer\n",
    "    pio.renderers.default = 'browser'\n",
    "    \n",
    "    # Create a color column for plotly\n",
    "    df_tsne_embeddings['color'] = df_tsne_embeddings['cluster'].map(\n",
    "        {cluster: mcolors.to_hex(cluster_color_map[cluster]) for cluster in unique_clusters}\n",
    "    )\n",
    "    \n",
    "    # Create the base figure with all clusters\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for cluster in sorted(unique_clusters):\n",
    "        cluster_data = df_tsne_embeddings[df_tsne_embeddings['cluster'] == cluster]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cluster_data['TSNE1'],\n",
    "            y=cluster_data['TSNE2'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=mcolors.to_hex(cluster_color_map[cluster]),\n",
    "                size=8,\n",
    "                line=dict(width=1, color='DarkSlateGrey')\n",
    "            ),\n",
    "            name=f'Cluster {cluster}',\n",
    "            text=cluster_data['company_name'],\n",
    "            hoverinfo='text',\n",
    "            hovertemplate='<b>%{text}</b><br>Cluster: ' + str(cluster) + '<extra></extra>',\n",
    "        ))\n",
    "    \n",
    "    # Define specific clusters to highlight\n",
    "    highlight_clusters = [\n",
    "        \"Türkiye İş Bankası A.Ş.\", \n",
    "        \"MLP Sağlık Hizmetleri A.Ş.\", \n",
    "        \"Turkuvaz Haberleşme ve Yayıncılık A.Ş.\", \n",
    "        \"Türkiye Sigorta A.Ş.\", \n",
    "        \"Doğuş Enerji\",\n",
    "        \"MUTLU AKÜ MALZ. SAN. A.Ş.\",\n",
    "        \"Eti Bakır A.Ş.\",\n",
    "        \"GÜRİŞ İNŞAAT VE MÜHENDİSLİK A.Ş.\",\n",
    "        \"Pınar Gıda Süt Ve Süt Ürünleri San. Tic. Ltd. Şti.\",\n",
    "        \"DESA DERİ SAN VE TİC AŞ\",\n",
    "        \"Yapı Kredi Bomonti Ada\"\n",
    "    ]\n",
    "    \n",
    "    # Function to calculate positions for annotations to avoid overlap\n",
    "    def get_annotation_positions(companies, max_labels=3):\n",
    "        \"\"\"\n",
    "        Determine positions for annotation labels to avoid overlap\n",
    "        \"\"\"\n",
    "        if len(companies) <= 0:\n",
    "            return []\n",
    "            \n",
    "        # Get only up to max_labels companies\n",
    "        if len(companies) > max_labels:\n",
    "            selected = companies.sample(max_labels, random_state=10) # 3, 4, 6, 7, \"10\"\n",
    "        else:\n",
    "            selected = companies\n",
    "        \n",
    "        # Calculate positions for each label\n",
    "        positions = []\n",
    "        for i, (_, company) in enumerate(selected.iterrows()):\n",
    "            # Create different positions based on the index\n",
    "            if i % 4 == 0:\n",
    "                ax, ay = 70, -40  # Right and up\n",
    "            elif i % 4 == 1:\n",
    "                ax, ay = -70, -40  # Left and up\n",
    "            elif i % 4 == 2:\n",
    "                ax, ay = 70, 40   # Right and down\n",
    "            else:\n",
    "                ax, ay = -70, 40  # Left and down\n",
    "                \n",
    "            positions.append({\n",
    "                'company': company,\n",
    "                'ax': ax,\n",
    "                'ay': ay\n",
    "            })\n",
    "            \n",
    "        return positions\n",
    "    \n",
    "    # Process each cluster and add annotations\n",
    "    for cluster_name in highlight_clusters:\n",
    "        # Get companies from this cluster\n",
    "        cluster_companies = df_tsne_embeddings[df_tsne_embeddings['cluster'] == cluster_name]\n",
    "        \n",
    "        if len(cluster_companies) == 0:\n",
    "            print(f\"Warning: No companies found in cluster '{cluster_name}'\")\n",
    "            continue\n",
    "        \n",
    "        # Get positions for annotations\n",
    "        annotation_positions = get_annotation_positions(cluster_companies)\n",
    "        \n",
    "        # Add annotations for each selected company\n",
    "        for position in annotation_positions:\n",
    "            company = position['company']\n",
    "            ax = position['ax']\n",
    "            ay = position['ay']\n",
    "            \n",
    "            # Truncate long company names\n",
    "            display_name = company['company_name']\n",
    "            if len(display_name) > 25:\n",
    "                display_name = display_name[:22] + \"...\"\n",
    "                \n",
    "            fig.add_annotation(\n",
    "                x=company['TSNE1'],\n",
    "                y=company['TSNE2'],\n",
    "                text=display_name,\n",
    "                showarrow=True,\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=1,\n",
    "                ax=ax,\n",
    "                ay=ay,\n",
    "                bgcolor=\"white\",\n",
    "                bordercolor=mcolors.to_hex(cluster_color_map[cluster_name]),\n",
    "                borderwidth=2,\n",
    "                font=dict(size=9),\n",
    "                opacity=0.9\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        margin=dict(l=0, r=0, t=30, b=0),\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        legend_title_text='Cluster'\n",
    "    )\n",
    "    \n",
    "    # If there are too many clusters, hide the legend\n",
    "    if num_clusters > 30:\n",
    "        fig.update_layout(showlegend=False)\n",
    "    \n",
    "    # Create a version without title for SVG export\n",
    "    fig_svg = go.Figure(fig)\n",
    "    fig_svg.update_layout(\n",
    "        title=None,\n",
    "        margin=dict(l=0, r=0, t=0, b=0)\n",
    "    )\n",
    "    \n",
    "    # Save as PDF\n",
    "    pio.write_image(fig_svg, \"tsne_clusters_with_labels.pdf\")\n",
    "    print(\"Visualization saved as 'tsne_clusters_with_labels.pdf'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA (reduce to 2 dimensions)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the embeddings\n",
    "pca_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a DataFrame with PCA components\n",
    "df_pca_embeddings = pd.DataFrame(pca_embeddings, columns=['PCA1', 'PCA2'])\n",
    "df_pca_embeddings['company_name'] = company_names\n",
    "\n",
    "# Set the same renderer\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "# Choose one of the existing colors from previous cluster color map (e.g., cluster 0)\n",
    "selected_color = mcolors.to_hex(cluster_color_map[\"Türkiye İş Bankası A.Ş.\"])\n",
    "\n",
    "# Create a PCA plot with consistent styling\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_pca_embeddings['PCA1'],\n",
    "    y=df_pca_embeddings['PCA2'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=selected_color,\n",
    "        size=8,\n",
    "        line=dict(width=1, color='DarkSlateGrey')\n",
    "    ),\n",
    "    text=df_pca_embeddings['company_name'],  # Optional hover info\n",
    "    hoverinfo='text',\n",
    "    hovertemplate='<b>%{text}</b><extra></extra>',\n",
    "    name='All Companies'\n",
    "))\n",
    "\n",
    "# Update layout to match style\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    margin=dict(l=0, r=0, t=30, b=0),\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create version without title for export\n",
    "fig_svg = go.Figure(fig)\n",
    "fig_svg.update_layout(\n",
    "    title=None,\n",
    "    margin=dict(l=0, r=0, t=0, b=0)\n",
    ")\n",
    "\n",
    "# Save to PDF\n",
    "pio.write_image(fig_svg, \"pca_visualization.pdf\")\n",
    "print(\"PCA visualization saved as 'pca_visualization.pdf'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clustered_df[\"metadata.cluster\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df[\"metadata.cluster\"].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648be075",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df[clustered_df[\"metadata.cluster\"] == \"DEARSAN GEMİ İNŞAAT SANAYİ A.Ş.\"][\"info.company_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b479d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
