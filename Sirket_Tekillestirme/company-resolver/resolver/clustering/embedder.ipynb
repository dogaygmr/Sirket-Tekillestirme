{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/shuffled_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = pd.json_normalize(data)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"completed_notes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to cache the embedded data\n",
    "embedding_cache = {}\n",
    "\n",
    "# Check for existing embedding files and load them into the cache\n",
    "if os.path.isfile(\"gte-multilingual-base.parquet\"):\n",
    "    embedding_cache['gte-multilingual-base'] = pd.read_parquet('gte-multilingual-base.parquet', engine='pyarrow')\n",
    "\n",
    "if os.path.isfile(\"paraphrase-multilingual-MiniLM-L12-v2.parquet\"):\n",
    "    embedding_cache[\"paraphrase-multilingual-MiniLM-L12-v2\"] = pd.read_parquet('paraphrase-multilingual-MiniLM-L12-v2.parquet', engine='pyarrow')\n",
    "\n",
    "if os.path.isfile(\"LaBSE.parquet\"):\n",
    "    embedding_cache[\"LaBSE\"] = pd.read_parquet('LaBSE.parquet', engine='pyarrow')\n",
    "\n",
    "if os.path.isfile(\"bge-m3.parquet\"):\n",
    "    embedding_cache[\"bge-m3\"] = pd.read_parquet('bge-m3.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(model_name, dataframe, columns_to_embed):\n",
    "    # Check if we already have embeddings for this model cached\n",
    "    if model_name.split(\"/\")[1] in embedding_cache:\n",
    "        print(f\"Using cached embeddings for model: {model_name.split('/')[1]}\")\n",
    "        return embedding_cache[model_name.split(\"/\")[1]]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load the sentence transformer model and move it to the GPU if available\n",
    "    model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "    \n",
    "    # Combine the text from specified columns into a single string per row\n",
    "    combined_texts = dataframe[columns_to_embed].astype(str).agg(' '.join, axis=1).tolist()\n",
    "    \n",
    "    # Generate the embeddings for the combined texts, and run them on the GPU if available\n",
    "    embeddings = model.encode(combined_texts, show_progress_bar=True, normalize_embeddings=True, device=device)\n",
    "\n",
    "    # Create a DataFrame with companies and embeddings\n",
    "    embedded_df = dataframe.copy()\n",
    "    embedded_df[\"metadata.embedding\"] = list(embeddings)\n",
    "\n",
    "    # Cache the embeddings for future use\n",
    "    embedding_cache[model_name.split(\"/\")[1]] = embedded_df\n",
    "\n",
    "    # Release the GPU memory by deleting the model and clearing the cache\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return embedded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"sentence-transformers/LaBSE\", \"Alibaba-NLP/gte-multilingual-base\", \n",
    "               \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"BAAI/bge-m3\"]\n",
    "\n",
    "columns_to_embed = [\"info.product_description\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    embedded_df = embed_data(model_name, data, columns_to_embed)\n",
    "    short_name = model_name.split(\"/\")[1]\n",
    "    embedded_df.to_parquet(f'{short_name}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_DBSCAN(dataframe, eps, metric):\n",
    "    # Create a copy of the original dataframe to avoid modifying it\n",
    "    df_copy = dataframe.copy()\n",
    "\n",
    "    # Extract the embeddings from the dataframe\n",
    "    embeddings = np.vstack(df_copy['metadata.embedding'].values)  # Stack the embeddings into a 2D array\n",
    "\n",
    "    # Perform DBSCAN clustering using the given distance metric\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=1, metric=metric)\n",
    "    df_copy['metadata.cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    # For each cluster, rename the cluster name to the first company in that cluster\n",
    "    cluster_names = {}\n",
    "    for cluster in df_copy['metadata.cluster'].unique():\n",
    "        # Find the first company name for the cluster\n",
    "        first_company = df_copy[df_copy['metadata.cluster'] == cluster].iloc[0]['info.company_name']\n",
    "        cluster_names[cluster] = first_company\n",
    "\n",
    "    # Replace cluster IDs with the first company name\n",
    "    df_copy['metadata.cluster'] = df_copy['metadata.cluster'].map(cluster_names)\n",
    "\n",
    "    # Return the dataframe with the relevant columns\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "def cluster_HDBSCAN(dataframe, min_cluster_size, min_samples, metric, eps):\n",
    "    df_copy = dataframe.copy()\n",
    "    embeddings = np.vstack(df_copy['metadata.embedding'].values).astype(np.float64)\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        distance_matrix = cosine_distances(embeddings).astype(np.float64)\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='precomputed',\n",
    "            gen_min_span_tree=True,\n",
    "            cluster_selection_method='eom',\n",
    "            cluster_selection_epsilon=eps\n",
    "        )\n",
    "        cluster_labels = clusterer.fit_predict(distance_matrix)\n",
    "    else:\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric=metric,\n",
    "            gen_min_span_tree=True,\n",
    "            cluster_selection_method='eom',\n",
    "            cluster_selection_epsilon=eps\n",
    "        )\n",
    "        cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    df_copy['cluster_id'] = cluster_labels\n",
    "\n",
    "    # Step 1: Separate clusters and outliers\n",
    "    clustered = df_copy[df_copy['cluster_id'] != -1].copy()\n",
    "    outliers = df_copy[df_copy['cluster_id'] == -1].copy()\n",
    "\n",
    "    # Step 2: Calculate cluster centroids\n",
    "    centroids = clustered.groupby('cluster_id')['metadata.embedding'].apply(\n",
    "        lambda x: np.mean(np.vstack(x.values), axis=0)\n",
    "    ).to_dict()\n",
    "\n",
    "    # Step 3: Assign each outlier to the closest centroid\n",
    "    for idx, row in outliers.iterrows():\n",
    "        embedding = np.array(row['metadata.embedding']).reshape(1, -1)\n",
    "        closest_cluster = min(\n",
    "            centroids.items(),\n",
    "            key=lambda item: cosine_distances(embedding, item[1].reshape(1, -1))[0][0]\n",
    "        )[0]\n",
    "        \n",
    "        # Assign this outlier to the closest cluster\n",
    "        df_copy.at[idx, 'cluster_id'] = closest_cluster\n",
    "\n",
    "        # Update centroid with new point (running mean)\n",
    "        old_centroid = centroids[closest_cluster]\n",
    "        cluster_points = df_copy[df_copy['cluster_id'] == closest_cluster]['metadata.embedding'].values\n",
    "        updated_centroid = np.mean(np.vstack(cluster_points), axis=0)\n",
    "        centroids[closest_cluster] = updated_centroid\n",
    "\n",
    "    # Step 4: Rename clusters by first company name\n",
    "    cluster_names = {}\n",
    "    for cluster in df_copy['cluster_id'].unique():\n",
    "        first_company = df_copy[df_copy['cluster_id'] == cluster].iloc[0]['info.company_name']\n",
    "        cluster_names[cluster] = first_company\n",
    "\n",
    "    df_copy['metadata.cluster'] = df_copy['cluster_id'].map(cluster_names)\n",
    "    df_copy.drop(columns=['cluster_id'], inplace=True)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cluster_members(df):\n",
    "  # Group the dataframe by 'cluster' and aggregate the company names into lists\n",
    "  cluster_members = df.groupby('metadata.cluster')['info.company_name'].apply(list).reset_index()\n",
    "    \n",
    "  # Rename the columns to 'cluster_name' and 'members'\n",
    "  cluster_members.columns = ['metadata.cluster', 'members']\n",
    "    \n",
    "  return cluster_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette_score(df, metric):\n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "    \n",
    "    if len(np.unique(labels)) == 1:\n",
    "        # Silhouette score cannot be computed with only one cluster\n",
    "        return -1\n",
    "    \n",
    "    score = silhouette_score(embeddings, labels, metric=metric)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gap_statistic(df, n_refs=5):\n",
    "    random_state = 42\n",
    "    \n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "\n",
    "    def calculate_dispersion_cosine(embeddings, labels):\n",
    "        \"\"\"Calculate the sum of squared cosine distances from points to their cluster centroids.\"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        dispersion = 0\n",
    "        for label in unique_labels:\n",
    "            cluster_points = embeddings[labels == label]\n",
    "            if len(cluster_points) > 1:\n",
    "                centroid = np.mean(cluster_points, axis=0)\n",
    "                # Compute cosine distances between points and their cluster centroids\n",
    "                distances = cosine_distances(cluster_points, centroid.reshape(1, -1))\n",
    "                dispersion += np.sum(distances ** 2)\n",
    "        return dispersion\n",
    "\n",
    "    # Calculate dispersion for actual data using cosine distances\n",
    "    actual_dispersion = calculate_dispersion_cosine(embeddings, labels)\n",
    "    \n",
    "    # Generate reference datasets and calculate their dispersion\n",
    "    ref_disps = np.zeros(n_refs)\n",
    "    for i in range(n_refs):\n",
    "        # Create a random reference dataset with a different seed for each run\n",
    "        np.random.seed(random_state + i)  # Different seed for each reference dataset\n",
    "        random_ref = np.random.uniform(low=np.min(embeddings, axis=0), high=np.max(embeddings, axis=0), size=embeddings.shape)\n",
    "        ref_kmeans = KMeans(n_clusters=len(np.unique(labels)), random_state=random_state).fit(random_ref)\n",
    "        ref_disps[i] = calculate_dispersion_cosine(random_ref, ref_kmeans.labels_)\n",
    "\n",
    "    # Calculate the gap statistic\n",
    "    gap_stat = np.log(np.mean(ref_disps)) - np.log(actual_dispersion)\n",
    "    return gap_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_sum_of_squares_cosine(df):\n",
    "    embeddings = np.vstack(df['metadata.embedding'].values)\n",
    "    labels = df['metadata.cluster']\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    total_ess = 0\n",
    "    for label in unique_labels:\n",
    "        cluster_points = embeddings[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            # Compute cosine distances from points to centroid\n",
    "            distances = cosine_distances(cluster_points, centroid.reshape(1, -1))\n",
    "            total_ess += np.sum(distances ** 2)\n",
    "    \n",
    "    return total_ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to return all three scores\n",
    "def run_tests(df, metric):\n",
    "    silhouette = calculate_silhouette_score(df, metric)\n",
    "    gap_stat = calculate_gap_statistic(df)\n",
    "    ess = calculate_error_sum_of_squares_cosine(df)\n",
    "    \n",
    "    return silhouette, gap_stat, ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_evaluate(model_name, dataframe, columns_to_embed, eps=0.1, metric='cosine'):\n",
    "    # Step 1: Embed the data\n",
    "    embedded_df = embed_data(model_name, dataframe, columns_to_embed)\n",
    "    \n",
    "    # Step 2: Cluster the embedded data using DBSCAN\n",
    "    #clustered_df = cluster_DBSCAN(embedded_df, eps=eps, metric=metric)\n",
    "    clustered_df = cluster_HDBSCAN(embedded_df, min_cluster_size=2, min_samples=1, metric=metric, eps=eps)\n",
    "    \n",
    "    # Step 3: Evaluate clustering with silhouette score, gap statistic, and error sum of squares\n",
    "    silhouette, gap_stat, ess = run_tests(clustered_df, metric)\n",
    "    \n",
    "    # Return the clustered dataframe along with the evaluation metrics\n",
    "    return clustered_df, silhouette, gap_stat, ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_embed = [\"info.product_description\"]\n",
    "\n",
    "model_names = [\"BAAI/bge-m3\", \"Alibaba-NLP/gte-multilingual-base\", \n",
    "               \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \"sentence-transformers/LaBSE\"]\n",
    "\n",
    "eps_values = [0, 0.00675, 0.0125, 0.025, 0.05, 0.1, 0.15, 0.2, 0.5, 1, 2, 4, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_and_eps(dataframe, columns_to_embed, model_names, eps_values):\n",
    "    results = []\n",
    "\n",
    "    # Outer loop for model names\n",
    "    for model_name in model_names:\n",
    "        # Inner loop for eps values\n",
    "        for eps in eps_values:\n",
    "            # Run the cluster_and_evaluate function with the current model and eps\n",
    "            clustered_df, silhouette, gap_stat, ess = cluster_and_evaluate(\n",
    "                model_name, dataframe, columns_to_embed, eps=eps\n",
    "            )\n",
    "            \n",
    "            # Append the results as a row in the list\n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'eps': eps,\n",
    "                'silhouette_score': silhouette,\n",
    "                'gap_statistic': gap_stat,\n",
    "                'error_sum_of_squares': ess\n",
    "            })\n",
    "    \n",
    "    # Convert the results list to a DataFrame and return it\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "results_df = evaluate_models_and_eps(data, columns_to_embed, model_names, eps_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df, silhouette, gap_stat, ess = cluster_and_evaluate(\"Alibaba-NLP/gte-multilingual-base\", data, columns_to_embed, eps=0, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Extract the embeddings and clusters\n",
    "embeddings = clustered_df['metadata.embedding'].tolist()\n",
    "clusters = clustered_df['metadata.cluster'].tolist()\n",
    "\n",
    "\n",
    "# Convert embeddings to a suitable format (if they're not already numpy arrays)\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "\n",
    "# Fit PCA on embeddings\n",
    "pca_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a new dataframe with the PCA components\n",
    "df_pca_embeddings = pd.DataFrame(pca_embeddings, columns=['PCA1', 'PCA2'])\n",
    "df_pca_embeddings['cluster'] = clusters  # Add the cluster information for coloring\n",
    "\n",
    "# Visualize the PCA results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', palette='viridis', data=df_pca_embeddings, s=100, edgecolor='k')\n",
    "plt.title('PCA of Embeddings')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Extract the embeddings and clusters\n",
    "embeddings = clustered_df['metadata.embedding'].tolist()\n",
    "clusters = clustered_df['metadata.cluster'].tolist()\n",
    "\n",
    "# Convert embeddings to a suitable format (if they're not already numpy arrays)\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "\n",
    "# Fit and transform the embeddings using t-SNE\n",
    "tsne_embeddings = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a new dataframe with the t-SNE components\n",
    "df_tsne_embeddings = pd.DataFrame(tsne_embeddings, columns=['TSNE1', 'TSNE2'])\n",
    "df_tsne_embeddings['cluster'] = clusters  # Add the cluster information for coloring\n",
    "\n",
    "# Visualize the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster', palette='viridis', data=df_tsne_embeddings, s=100, edgecolor='k')\n",
    "plt.title('t-SNE of Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clustered_df[\"metadata.cluster\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df[\"metadata.cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df[clustered_df[\"metadata.cluster\"] == \"Sanofi Sağlık Ürünleri Limited Şirketi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings and clusters\n",
    "embeddings = clustered_df['metadata.embedding'].tolist()\n",
    "clusters = clustered_df['metadata.cluster'].tolist()\n",
    "\n",
    "\n",
    "# Convert embeddings to a suitable format (if they're not already numpy arrays)\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "\n",
    "# Fit PCA on embeddings\n",
    "pca_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a new dataframe with the PCA components\n",
    "df_pca_embeddings = pd.DataFrame(pca_embeddings, columns=['PCA1', 'PCA2'])\n",
    "df_pca_embeddings['cluster'] = clusters  # Add the cluster information for coloring\n",
    "\n",
    "# Visualize the PCA results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', palette='viridis', data=df_pca_embeddings, s=100, edgecolor='k')\n",
    "plt.title('PCA of Embeddings')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend().set_visible(False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings and clusters\n",
    "embeddings = clustered_df['metadata.embedding'].tolist()\n",
    "clusters = clustered_df['metadata.cluster'].tolist()\n",
    "\n",
    "# Convert embeddings to a suitable format (if they're not already numpy arrays)\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "\n",
    "# Fit and transform the embeddings using t-SNE\n",
    "tsne_embeddings = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create a new dataframe with the t-SNE components\n",
    "df_tsne_embeddings = pd.DataFrame(tsne_embeddings, columns=['TSNE1', 'TSNE2'])\n",
    "df_tsne_embeddings['cluster'] = clusters  # Add the cluster information for coloring\n",
    "\n",
    "# Visualize the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster', palette='viridis', data=df_tsne_embeddings, s=100, edgecolor='k')\n",
    "plt.title('t-SNE of Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
